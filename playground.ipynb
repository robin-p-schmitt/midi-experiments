{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T08:57:22.377374Z",
     "start_time": "2025-01-17T08:57:22.149977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "import data_utils\n",
    "importlib.reload(data_utils)"
   ],
   "id": "3a7584434417a755",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_utils' from '/mnt/projects/2025_16_01_midi_exps/data_utils.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T09:09:37.275257Z",
     "start_time": "2025-01-17T09:09:37.064779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset import get_data_loaders_for_training\n",
    "from data_utils import HDF_FILE_PATH\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "\n",
    "_, _, dev_loader = get_data_loaders_for_training(HDF_FILE_PATH, batch_size, num_workers)"
   ],
   "id": "3a78f275c205b028",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T09:36:12.376336Z",
     "start_time": "2025-01-17T09:36:11.516991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data_utils import visualize_piano_roll\n",
    "from models.transformer import TransformerDecoderModel, MODEL_CHECKPOINTS_PATH\n",
    "from models.configs import transformer_v1_config\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "N_CONTEXT_FRAMES = 200\n",
    "N_PREDICTION_STEPS = 300\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TransformerDecoderModel(**transformer_v1_config[\"model_opts\"])\n",
    "model.load_state_dict(torch.load(f\"{MODEL_CHECKPOINTS_PATH}/transformer_v1_boosted-mse.pt\"))\n",
    "model.to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def piano_rolls_tensor_to_numpy(piano_rolls_tensor: torch.Tensor):\n",
    "  return piano_rolls_tensor.permute(0, 2, 1)[0].numpy()\n",
    "\n",
    "\n",
    "for batch_idx, piano_rolls in enumerate(dev_loader):\n",
    "    visualize_piano_roll(\n",
    "        piano_rolls_tensor_to_numpy(piano_rolls),\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # context frames to start prediction\n",
    "        piano_rolls_context = piano_rolls[:, :N_CONTEXT_FRAMES].to(DEVICE)\n",
    "        # create tensor to hold the predicted tensor\n",
    "        predicted_piano_rolls = torch.zeros((piano_rolls.size(0), N_CONTEXT_FRAMES + N_PREDICTION_STEPS + 1, piano_rolls.size(2)), device=DEVICE)\n",
    "        # insert context into tensor while considering initial 0 padding frame\n",
    "        predicted_piano_rolls[:, 1:N_CONTEXT_FRAMES+1] = piano_rolls_context\n",
    "\n",
    "        for step in range(N_PREDICTION_STEPS):\n",
    "            outputs = model(predicted_piano_rolls[:, :N_CONTEXT_FRAMES + step + 1])\n",
    "            predicted_piano_rolls[:, N_CONTEXT_FRAMES + step + 1] = outputs[:, -1]\n",
    "\n",
    "        predicted_piano_rolls = predicted_piano_rolls[:, :-1].to(\"cpu\")\n",
    "\n",
    "    visualize_piano_roll(\n",
    "        piano_rolls_tensor_to_numpy(predicted_piano_rolls),\n",
    "    )\n",
    "\n",
    "    loss = criterion(predicted_piano_rolls, piano_rolls)  # Example target as input\n",
    "    print(\"Loss: \", loss)\n",
    "    \n",
    "    error = predicted_piano_rolls - piano_rolls\n",
    "    error[piano_rolls != 0] *= 2\n",
    "    loss_boosted = torch.mean(error ** 2)\n",
    "    print(\"loss_boosted: \", loss_boosted)\n",
    "\n",
    "    break"
   ],
   "id": "df1c8c9f92c07cb2",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_checkpoints/transformer_v1_boosted-mse.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m DEVICE \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m TransformerDecoderModel(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtransformer_v1_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_opts\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m---> 12\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mMODEL_CHECKPOINTS_PATH\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/transformer_v1_boosted-mse.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     13\u001B[0m model\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[1;32m     14\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mMSELoss()\n",
      "File \u001B[0;32m/mnt/miniconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/serialization.py:986\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[1;32m    983\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    984\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 986\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _open_file_like(f, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[1;32m    987\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[1;32m    988\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[1;32m    989\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[1;32m    990\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[1;32m    991\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[0;32m/mnt/miniconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/serialization.py:435\u001B[0m, in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    433\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[1;32m    434\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[0;32m--> 435\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _open_file(name_or_buffer, mode)\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    437\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[0;32m/mnt/miniconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/serialization.py:416\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[0;32m--> 416\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mopen\u001B[39m(name, mode))\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'model_checkpoints/transformer_v1_boosted-mse.pt'"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
